arrange(desc(pct_anymembershp_zip)) %>%
filter(pct_anymembershp_zip >= 0.455563) %>%
filter(income_per_capita <= 22653.77)
median(smith$nbanks_zip, na.rm = TRUE) #0.455563
median(smith$census_response_rate2020, na.rm = TRUE) #5
summary(smith$census_response_rate2020, na.rm = TRUE)
summary(smith$nbanks_zip, na.rm = TRUE) #5
summary(smith$pct_anymembershp_zip, na.rm = TRUE) #0.455563
#communities in the upper quartile of census response - 65%- but less than median income - $22,654
smith2 %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, nbanks_zip) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(income_per_capita <= 22653.77)
#communities in the upper quartile of census response - 65%- but less than median income - $22,654
smith2 %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, nbanks_zip) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(income_per_capita <= 22653.77)
#communities in the upper quartile of census response - 65%- and upper quartile of membership - 46%
smith2 %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, nbanks_zip) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(pct_anymembershp_zip >= 0.4613)
smith2 %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, nbanks_zip) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(pct_anymembershp_zip >= 0.4613) %>%
arrange(desc(income_per_capita))
#communities in the upper quartile of census response - 65%- and upper quartile of membership - 46%
smith2 %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, nbanks_zip) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(pct_anymembershp_zip >= 0.4613) %>%
arrange((income_per_capita))
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
download_loc <- "../data/newspapers_list.txt"
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", donwload_loc)
download_loc <- "../data/newspapers_list.txt"
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", download_loc)
newspaper_list <- read_delim(download_loc, delim = "|")
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate_all(str_squish) %>%
mutate(earliest_issue = mdy(earliest_issue)) %>%
mutate(latest_issue = mdy(latest_issue))
download_loc <- "../data/newspapers_list.txt"
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", download_loc)
newspaper_list <- read_delim(download_loc, delim = "|")
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate_all(str_squish) %>%
mutate(earliest_issue = lubridate::mdy(earliest_issue)) %>%
mutate(latest_issue = lubridate::mdy(latest_issue))
View(clean_newspaper_list)
new_main_index <- read.csv("../data/mainindex_10_30.csv")
View(new_main_index)
ga_papers_years <- new_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name, year) %>%
count()
View(ga_papers_years)
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
View(tolnay_beck)
beck_lynchings <- tolnay_beck %>%
filter(!str_detect(status, "death"))
beck_by_state <- beck_lynchings %>%
group_by(lynch_state) %>%
count(.) %>%
rename(state_lynch = lynch_state) %>%
rename(beck_count = n) %>%
mutate(percent_lynchings_beck = round(beck_count/nrow(beck_lynchings)*100,2))
View(beck_by_state)
#install.packages("here")
#here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#install.packages("geosphere")
library(geosphere)
library(janitor)
#install.packages('scales')
library(scales)
lynch_updated <- read_csv("../data/lynch_geocoded_10.8.csv")
# make less ugly
lynch_updated <- lynch_updated %>%
clean_names()
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
mutate(date = paste(month,day,year, sep = "/")) %>%
mutate(tolnay_date = as.Date(date, "%m/%d/%Y")) %>%
mutate(date_est = tolnay_date - 5) %>%
select(status, tolnay_date, year, month, day, name, alt_name_1, lynch_county, lynch_state, method_of_death, accusation, date_est) %>%
filter(!is.na(date_est))
tolnay_beck <- read.csv("../data/bailey_beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
lynch_updated <- read_csv("../data/lynch_geocoded_10.8.csv")
# make less ugly
lynch_updated <- lynch_updated %>%
clean_names()
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
mutate(date = paste(month,day,year, sep = "/")) %>%
mutate(tolnay_date = as.Date(date, "%m/%d/%Y")) %>%
mutate(date_est = tolnay_date - 5) %>%
select(status, tolnay_date, year, month, day, name, alt_name_1, lynch_county, lynch_state, method_of_death, accusation, date_est) %>%
filter(!is.na(date_est))
View(ga_tolnay)
View(new_main_index)
View(newspaper_list)
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, LCCN, OCLC,ISSN, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate_all(str_squish) %>%
mutate(earliest_issue = lubridate::mdy(earliest_issue)) %>%
mutate(latest_issue = lubridate::mdy(latest_issue))
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, lccn, oclc, issn, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate_all(str_squish) %>%
mutate(earliest_issue = lubridate::mdy(earliest_issue)) %>%
mutate(latest_issue = lubridate::mdy(latest_issue))
write_csv(clean_newspaper_list, "../data/newspaper_list.csv")
filtered_main_index <- clean_main_index %>%
filter(is.na(newspaper_state_clean)) %>%
filter(!is.na(newspaper_city))
filtered_main_index <- new_main_index %>%
filter(is.na(newspaper_state_clean)) %>%
filter(!is.na(newspaper_city))
View(filtered_main_index)
filtered_main_index <- new_main_index %>%
filter(is.na(newspaper_state_clean))
names(clean_newspaper_list)
names(filtered_main_index)
fixed <- clean_newspaper_list %>%
inner_join(filtered_main_index, by=c("lccn"=="sn"))
fixed <- clean_newspaper_list %>%
inner_join(filtered_main_index, by=c("lccn"="sn"))
View(fixed)
fixed <- clean_newspaper_list %>%
inner_join(filtered_main_index, by=c("lccn"="sn")) %>%
distinct()
main_index_2 <- clean_newspaper_list %>%
inner_join(new_main_index, by=c("lccn"="sn")) %>%
distinct()
names(clean_newspaper_list)
names(new_main_index)
main_index_2 <- clean_newspaper_list %>%
inner_join(new_main_index, by=c("lccn"="sn", "newspaper"="newspaper_name")) %>%
distinct()
main_index_2 <- clean_newspaper_list %>%
inner_join(new_main_index, by=c("lccn"="sn", "state"="newspaper_state")) %>%
distinct()
fixed_anti <- clean_newspaper_list %>%
anti_join(filtered_main_index, by=c("lccn"="sn"))
View(fixed_anti)
20529-20226
62814-60042
x <- main_index_2 %>%
anti_join(new_main_index, by=c("lccn"))
x <- main_index_2 %>%
anti_join(new_main_index, by=c("lccn"="sn"))
names(new_main_index)
names(main_index_2)
main_index_2 <- clean_newspaper_list %>%
inner_join(new_main_index, by=c("lccn"="sn")) %>%
distinct()
x <- main_index_2 %>%
anti_join(new_main_index, by=c("lccn"="sn"))
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda")
library(quanteda)
#import df
#lynch <- read_csv("articles_df.csv")
lynch <- read_csv("../data/articles_oct_19.csv")
the1890s <-  lynch %>%
filter(year >= 1890 & year <=1899)
the1890s %>%
select(filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(filename) %>%
summarize(total =sum(n))
#1637 articles
statesthe1890s <- the1890s %>%
select(newspaper_state, filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(newspaper_state) %>%
arrange(desc(n))
statesthe1890s %>%
select(newspaper_state, n) %>%
slice_max(n, n=10)
#newspaper_state
# Kansas	115
# Wisconsin	102
# Missouri	86
# Kentucky	83
# Minnesota	83
# North Dakota	75
# Georgia	73
# South Dakota	71
# North Carolina	57
# Mississippi	54
#Fact Check
#sum(statesthe1850s$n)
x <- the1890s %>%
distinct(filename, .keep_all = TRUE) %>%
arrange(date)
#write_csv(x, "../output/the1890s_index.csv")
the1900s <-  lynch %>%
filter(year >= 1900 & year <=1909)
the1900s %>%
select(filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(filename) %>%
summarize(total =sum(n))
#1637 articles
statesthe1900s <- the1900s %>%
select(newspaper_state, filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(newspaper_state) %>%
arrange(desc(n))
statesthe1900s %>%
select(newspaper_state, n) %>%
slice_max(n, n=10)
#newspaper_state
# Kansas	115
# Wisconsin	102
# Missouri	86
# Kentucky	83
# Minnesota	83
# North Dakota	75
# Georgia	73
# South Dakota	71
# North Carolina	57
# Mississippi	54
#Fact Check
#sum(statesthe1850s$n)
x <- the1900s %>%
distinct(filename, .keep_all = TRUE) %>%
arrange(date)
write_csv(x, "../output/the1900s_index.csv")
stories <- str_replace_all(the1900s$sentence, "- ", "")
stories_df <- tibble(stories,)
# unnest includes lower, punct removal
stories_tokenized <- stories_df %>%
unnest_tokens(word,stories)
stories_tokenized
#Remove stopwords
data(stop_words)
stories_tokenized <- stories_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
#NOT SURE IF THIS LINE SHOULD REMAIN
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# fix the script so it doesn't pick up these file names, numbers
# forcibly removing for now
# Word Count
story_word_ct <- stories_tokenized %>%
count(word, sort=TRUE)
#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")
stories_bigrams <- stories_df %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
stories_bigrams
#Filter out stop words.
stories_bigrams_separated <- stories_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
stories_bigrams_filtered <- stories_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
stories_bigram_cts <- stories_bigrams_filtered %>%
count(word1, word2, sort = TRUE)
# put back into bigram form if we want to use them
stories_bigrams_united <- stories_bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#replace Date for the decade analyzed
stories_bigram_cts_1900s <- stories_bigram_cts %>%
mutate(decade = "1900")
write_csv(stories_bigram_cts_1900s, "../output/1900s_lynch_bigram_count.csv")
stories_trigrams <- stories_df %>%
unnest_tokens(trigram, stories, token="ngrams", n=3)
stories_trigrams_separated <- stories_trigrams %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
stories_trigrams_ct <- stories_trigrams_separated %>%
count(word1, word2, word3, sort = TRUE)
#filtered
# stories_trigrams_filtered <- stories_trigrams_separated %>%
#   filter(!word1 %in% stop_words$word) %>%
#   filter(!word2 %in% stop_words$word) %>%
#   filter(!word3 %in% stop_words$word)
#
# stories_trigrams_ct <- stories_trigrams_filtered %>%
#   count(word1, word2, word3, sort = TRUE)
#replace Date for the decade analyzed
stories_trigrams_ct_1900s <- stories_trigrams_ct %>%
mutate(decade = "1900")
write_csv(stories_trigrams_ct_1900s, "../output/1900s_lynch_trigram_count.csv")
the1910s <-  lynch %>%
filter(year >= 1910 & year <=1919)
the1910s %>%
select(filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(filename) %>%
summarize(total =sum(n))
#1627 articles
statesthe1910s <- the1910s %>%
select(newspaper_state, filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(newspaper_state) %>%
arrange(desc(n))
statesthe1910s %>%
select(newspaper_state, n) %>%
slice_max(n, n=10)
#newspaper_state
# Wisconsin	88
# Mississippi	87
# Nebraska	83
# Missouri	77
# Utah	76
# Arkansas	72
# North Carolina	69
# South Dakota	66
# Kentucky	64
# Alabama	62
#Fact Check
#sum(statesthe1850s$n)
x <- the1910s %>%
distinct(filename, .keep_all = TRUE) %>%
arrange(date)
write_csv(x, "../output/the1910s_index.csv")
stories <- str_replace_all(the1910s$sentence, "- ", "")
stories_df <- tibble(stories,)
# unnest includes lower, punct removal
stories_tokenized <- stories_df %>%
unnest_tokens(word,stories)
stories_tokenized
#Remove stopwords
data(stop_words)
stories_tokenized <- stories_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
#NOT SURE IF THIS LINE SHOULD REMAIN
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# fix the script so it doesn't pick up these file names, numbers
# forcibly removing for now
# Word Count
story_word_ct <- stories_tokenized %>%
count(word, sort=TRUE)
#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")
stories_bigrams <- stories_df %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
stories_bigrams
#Filter out stop words.
stories_bigrams_separated <- stories_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
stories_bigrams_filtered <- stories_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
stories_bigram_cts <- stories_bigrams_filtered %>%
count(word1, word2, sort = TRUE)
# put back into bigram form if we want to use them
stories_bigrams_united <- stories_bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#replace Date for the decade analyzed
stories_bigram_cts_1910s <- stories_bigram_cts %>%
mutate(decade = "1910")
write_csv(stories_bigram_cts_1910s, "../output/1910s_lynch_bigram_count.csv")
stories_trigrams <- stories_df %>%
unnest_tokens(trigram, stories, token="ngrams", n=3)
stories_trigrams_separated <- stories_trigrams %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
stories_trigrams_ct <- stories_trigrams_separated %>%
count(word1, word2, word3, sort = TRUE)
#filtered
# stories_trigrams_filtered <- stories_trigrams_separated %>%
#   filter(!word1 %in% stop_words$word) %>%
#   filter(!word2 %in% stop_words$word) %>%
#   filter(!word3 %in% stop_words$word)
#
# stories_trigrams_ct <- stories_trigrams_filtered %>%
#   count(word1, word2, word3, sort = TRUE)
#replace Date for the decade analyzed
stories_trigrams_ct_1910s <- stories_trigrams_ct %>%
mutate(decade = "1910")
write_csv(stories_trigrams_ct_1910s, "../output/1910s_lynch_trigram_count.csv")
the1920s <-  lynch %>%
filter(year >= 1920 & year <=1929)
the1920s %>%
select(filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(filename) %>%
summarize(total =sum(n))
#840 articles
statesthe1920s <- the1920s %>%
select(newspaper_state, filename) %>%
distinct(filename, .keep_all = TRUE) %>%
count(newspaper_state) %>%
arrange(desc(n))
statesthe1920s %>%
select(newspaper_state, n) %>%
slice_max(n, n=10)
#newspaper_state
# Alaska	85
# Arkansas	68
# Nebraska	56
# Illinois	41
# Alabama	34
# Wisconsin	29
# North Dakota	27
# Colorado	23
# Texas	23
# West Virginia	23
#Fact Check
#sum(statesthe1850s$n)
x <- the1920s %>%
distinct(filename, .keep_all = TRUE) %>%
arrange(date)
write_csv(x, "../output/the1920s_index.csv")
stories <- str_replace_all(the1920s$sentence, "- ", "")
stories_df <- tibble(stories,)
# unnest includes lower, punct removal
stories_tokenized <- stories_df %>%
unnest_tokens(word,stories)
stories_tokenized
#Remove stopwords
data(stop_words)
stories_tokenized <- stories_tokenized %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(word != "temp_file") %>%
#NOT SURE IF THIS LINE SHOULD REMAIN
filter(word != "stories_corpus") %>%
filter(!grepl('[0-9]', word))
# fix the script so it doesn't pick up these file names, numbers
# forcibly removing for now
# Word Count
story_word_ct <- stories_tokenized %>%
count(word, sort=TRUE)
#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")
stories_bigrams <- stories_df %>%
unnest_tokens(bigram, stories, token="ngrams", n=2)
stories_bigrams
#Filter out stop words.
stories_bigrams_separated <- stories_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
stories_bigrams_filtered <- stories_bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
stories_bigram_cts <- stories_bigrams_filtered %>%
count(word1, word2, sort = TRUE)
# put back into bigram form if we want to use them
stories_bigrams_united <- stories_bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
#replace Date for the decade analyzed
stories_bigram_cts_1920s <- stories_bigram_cts %>%
mutate(decade = "1920")
write_csv(stories_bigram_cts_1920s, "../output/1920s_lynch_bigram_count.csv")
stories_trigrams <- stories_df %>%
unnest_tokens(trigram, stories, token="ngrams", n=3)
stories_trigrams_separated <- stories_trigrams %>%
separate(trigram, c("word1", "word2", "word3"), sep = " ")
stories_trigrams_ct <- stories_trigrams_separated %>%
count(word1, word2, word3, sort = TRUE)
#filtered
# stories_trigrams_filtered <- stories_trigrams_separated %>%
#   filter(!word1 %in% stop_words$word) %>%
#   filter(!word2 %in% stop_words$word) %>%
#   filter(!word3 %in% stop_words$word)
#
# stories_trigrams_ct <- stories_trigrams_filtered %>%
#   count(word1, word2, word3, sort = TRUE)
#replace Date for the decade analyzed
stories_trigrams_ct_1920s <- stories_trigrams_ct %>%
mutate(decade = "1920")
write_csv(stories_trigrams_ct_1920s, "../output/1920s_lynch_trigram_count.csv")
