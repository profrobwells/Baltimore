model             = "base",
options           = list(seed = 250)
)
top_words(out)  # top words can aid selecting keywords
out <- keyATM(
docs              = docs_withSplit,  # 70% of the corpus
no_keyword_topics = 5,               # number of topics without keywords
keywords          = keywords,        # selected keywords
model             = "base",          # select the model
options           = list(seed = 250)
)
save(out, file = "../output/keyATM_topic_model.rds")
top_words(out)
plot_topicprop(out, show_topic = 1:5)
plot<- plot_topicprop(out, show_topic = 1:5)
save_fig(plot, "../output/plot_keyword.pdf", width = 6.5, height = 4)
top_docs(out)
plot_alpha(out)
plot_pi(out)
#install.packages("here")
here::here()
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda")
library(quanteda)
black_papers <- read_csv("../data/black_papers.csv") %>%
rename(newspaper_name = Title) %>%
as.data.frame()
library(tidyverse)
#install.packages('textdata')
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(googlesheets4)
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 3500 articles
lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 7162 articles
lynch <- read_csv("../data/articles_oct_19.csv")
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, URL)
View(lynch)
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, url)
names(lynch)
lynch  <- lynch [ -c(22) ]
write.csv(lynch, ("../data/articles_oct_19.csv"))
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, url)
View(x)
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 7162 articles
lynch <- read_csv("../data/articles_oct_19.csv")
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 7162 articles
lynch <- read.csv("../data/articles_oct_19.csv")
View(lynch)
View(x)
z <- lynch %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
View(lynch)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, url)
#
# # append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
z$file_id <- gsub('.txt',"", z$filename)
library(tidyr)
z <- separate(data = z, col = filename, into = c('fn1', 'crap'), sep = '_', extra ='merge', fill = 'right')
z$fn1 <- as.numeric(z$fn1)
z <- subset(z, select =-crap)
z <- y %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
#Count by decade, filter off 1960 as noise
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade==1960)
View(lynch_word_decade)
View(z)
6448+714
#76020 rows of BP text data for tokenization
bp_text <- read.csv("../data/black_press_article_text_oct_19.csv")
#loads text of 3500 articles
#lynch <- read_csv("../data/articles_aug_25.csv")
#loads text of 6448 articles predominantly white articles
white_lynch <- read.csv("../data/articles_oct_19.csv")
#loads xxx join 714 articles + 358 LOC articles xxx black press articles
#76020 rows of BP text data for tokenization
bp_text <- read.csv("../data/black_press_article_text_oct_19.csv")
View(bp_text)
black <- bp_text
xx <- stringi::stri_count_words(black$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
black <- cbind(black, xx)
yy <- black %>%
select(filename, sentence, words, year)
# append decade information for aggregation
yy$decade <- paste0(substr(yy$year, 0, 3), "0")
zz <- yy %>%
select(filename, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
#Count words by decade
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
#Count articles by year
black_articles <- blackindex %>%
select(filename, year) %>%
group_by(year) %>%
count()
#1045 black press articles. join 714 articles + 358 LOC articles
blackindex_master <- read.csv("../data/blackindex_master.csv")
black_articles <- blackindex_master %>%
select(filename, year) %>%
group_by(year) %>%
count()
View(black_articles)
black_articles_decade <- zz %>%
select(decade, filename) %>%
group_by(decade) %>%
count() %>%
mutate(Pct_Total =formattable::percent(round(n/714,2)))
View(black_articles_decade)
sum(black_articles_decade$n)
165/1045
black_articles_decade <- zz %>%
select(decade, filename) %>%
group_by(decade) %>%
count() %>%
mutate(Pct_Total =formattable::percent(round(n/1045,1)))
black_articles_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
library(kableExtra)
black_articles_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_articles_decade <- zz %>%
select(decade, filename) %>%
group_by(decade) %>%
count() %>%
mutate(Pct_Total =formattable::percent(round(n/1045,2)))
library(kableExtra)
black_articles_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_word_decade %>%
ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Black Press Avg Word Count in Lynching News Coverage",
subtitle = "Based in 1,045 extracted articles, 1892-2002",
caption = "Graphic by Rob Wells, 12-26-2023",
y="Average Article Word Count",
x="Decade")
#ggsave("../output_images_tables/FigureX_black_press_avg_word_count_ap_19.png",device = "png",width=9,height=6, dpi=800)
View(zz)
View(black)
onlybptext <- filter(yy, grepl("bp", filename))
View(onlybptext)
write.csv(onlybptext("../data/only_bp_text.csv"))
write_csv(onlybptext("../data/only_bp_text.csv"))
write.csv(onlybptext,("../data/only_bp_text.csv"))
b <- stringi::stri_count_words(onlybptext$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
View(b)
onlybytext <- cbind(onlybptext, b)
bb <- onlybytext %>%
select(filename, sentence, words, year, newspaper_name, url)
names(onlybptext)
#65257 rows only the bp files.
onlybptext <- filter(black, grepl("bp", filename))
write.csv(onlybptext,("../data/only_bp_text.csv"))
b <- stringi::stri_count_words(onlybptext$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
onlybytext <- cbind(onlybptext, b)
bb <- onlybytext %>%
select(filename, sentence, words, year, newspaper_name, url)
names(onlybptext)
bb <- onlybptext %>%
select(filename, sentence, words, year, newspaper_name, url)
#
# # append decade information for aggregation
bb$decade <- paste0(substr(bb$year, 0, 3), "0")
yyy <- cbind(bb, y)
yyy <- rbind(bb, y)
zzz <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
zzz$file_id <- gsub('.txt',"", zzz$filename)
library(tidyr)
zzz <- separate(data = zzz, col = filename, into = c('fn1', 'crap'), sep = '_', extra ='merge', fill = 'right')
zzz$fn1 <- as.numeric(zzz$fn1)
zzz <- subset(zzz, select =-crap)
View(zzz)
yyy <- rbind(bb, y)
#MAKE A NEW DF THAT GROUPS BY FILE NAME, SUMMARIZES WORD COUNT SO EACH ARTICLE HAS A WORD COUNT
zzz <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
View(zzz)
z <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(z)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade==1960)
View(lynch_word_decade)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
View(zzz)
View(yyy)
lynch_word_decade %>%
ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Average Word Count in Lynching News Coverage",
subtitle = "Based in 7,162 extracted articles, 1805-1969",
caption = "Graphic by Rob Wells, 12-26-2023",
y="Average Article Word Count",
x="Decade")
# ggsave("../output_images_tables/FigureX_avg_word_count_ap_16.png",device = "png",width=9,height=6, dpi=800)
ggsave("../output_images_tables/FigureX_avg_word_count_dec_26.png",device = "png",width=9,height=6, dpi=800)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = count(decade))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = summarise(count(decade)))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = summarise(count=decade()))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = count=decade())
glimpse(z)
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
group_by(decade) %>%
mutate(stories_decade = count(decade)) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
# group_by(decade) %>%
mutate(stories_decade = count(decade)) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
# group_by(decade) %>%
mutate(stories_decade = count(decade))
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
)
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
) %>%
mutate(avg_words = round(avg_words, 0))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
View(lynch_word_decade)
mean(lynch_word_decade$avg_words)
median(lynch_word_decade$avg_words)
black <- bp_text
xx <- stringi::stri_count_words(black$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
black <- cbind(black, xx)
View(black)
yy <- black %>%
select(filename, sentence, words, year)
# append decade information for aggregation
yy$decade <- paste0(substr(yy$year, 0, 3), "0")
zz <- yy %>%
select(filename, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0))
View(black_word_decade)
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
mean(black_word_decade$avg_words)
sum(black_word_decade$num_articles)
black_word_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0))
library(kableExtra)
# black_articles_decade %>%
black_word_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
library(reticulate)
reticulate::repl_python()
py_install("boto3")
reticulate::repl_python()
py_install("pillow")
reticulate::repl_python()
install.packages("retriculate")
library(reticulate)
install.packages("reticulate")
library(reticulate)
py_install("pillow")
# use_python("/path/to/your/python")
reticulate::repl_python()
library(reticulate)
py_install("pillow")
# use_python("/path/to/your/python")
reticulate::repl_python()
setwd("~/Code/data_journalism_class/03_tutorials/qmd_files")
quarto_render
quarto::quarto_render()
getwd()
setwd("~/Code/Baltimore/code")
#remotes::install_github("walkerke/tidycensus")
library(tidyverse)
library(tidycensus)
#a = get_decennial(geography = "state", variables = "P1_001N", year = 2020)
# head(a, 5)
#install.packages("formattable")
library(formattable)
library(htmlwidgets)
library(leaflet)
library(sf)
library(formattable)
library(dplyr)
library(tidyr)
library(janitor)
#install.packages("leaflet.extras")
library(leaflet.extras)
library(googlesheets4)
census_api_key("9cabe8a191a1f824755d4a1845f13cb08faa2c5f", install = TRUE, overwrite = TRUE)
smith <- read.csv("../data/smith1_balt.csv")
#199 census tracts
#25 zip codes (totalling 199 areas, but need to check boundaries)
x<- smith %>%
group_by(zip) %>%
count ()
sum(x$n)
#per capita income seems way too low
median(smith$income_per_capita, na.rm = TRUE) #22789.76
median(smith$pct_anymembershp_zip, na.rm = TRUE) #0.455563
summary(smith$pct_anymembershp_zip, na.rm = TRUE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
#  0.3353  0.4444  0.4556  0.4613  0.4752  0.5241       3
median(smith$nbanks_zip, na.rm = TRUE) #5
summary(smith$nbanks_zip, na.rm = TRUE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
# 0.000   2.000   5.000   5.847   8.000  14.000       3
median(smith$census_response_rate2020, na.rm = TRUE) #0.564
summary(smith$census_response_rate2020, na.rm = TRUE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
#  0.2830  0.4753  0.5640  0.5594  0.6522  0.8360       3
View(smith)
#communities with greater than median membership - 46%- but less than median income - $22,790
himember <- smith %>%
group_by(neighborhood) %>%
select(pct_anymembershp_zip, income_per_capita, tract) %>%
arrange(desc(pct_anymembershp_zip)) %>%
filter(pct_anymembershp_zip >= 0.455563) %>%
filter(income_per_capita <= 22789.76) %>%
mutate(pct_anymembershp_zip = formattable::percent(pct_anymembershp_zip)) %>%
himember$income_per_capita <- formattable::currency(himember$income_per_capita, digits =0L)
#communities with greater than median membership - 46%- but less than median income - $22,790
himember <- smith %>%
group_by(neighborhood) %>%
select(pct_anymembershp_zip, income_per_capita, tract) %>%
arrange(desc(pct_anymembershp_zip)) %>%
filter(pct_anymembershp_zip >= 0.455563) %>%
filter(income_per_capita <= 22789.76) %>%
mutate(pct_anymembershp_zip = formattable::percent(pct_anymembershp_zip))
himember$income_per_capita <- formattable::currency(himember$income_per_capita, digits =0L)
library(kableExtra)
himember %>%
rename(Neighborhood = neighborhood, Membership = pct_anymembershp_zip, Per_Capita_Income = income_per_capita, Census_Tract = tract) %>%
kbl(caption = "Communities With Greater than Median Membership - 46%- but Less than Median Income - $22,790", font_size = 50, bold=T) %>%
kable_classic(full_width = T, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, border_right = T, width = "10em", background = "yellow") %>%
column_spec(3, border_right = T, width = "10em") %>%
save_kable(file = "../DataOutput/himember.html", self_contained = T)
hicensus <- smith %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, tract) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(income_per_capita <= 22653.77) %>%
mutate(census_response_rate2020 = formattable::percent(census_response_rate2020)) %>%
mutate(pct_anymembershp_zip = formattable::percent(pct_anymembershp_zip))
hicensus$income_per_capita <- formattable::currency(hicensus$income_per_capita, digits =0L)
#communities in the upper quartile of census response - 65%- and upper quartile of membership - 46%
smith %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, nbanks_zip) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(pct_anymembershp_zip >= 0.4613) %>%
arrange((income_per_capita))
