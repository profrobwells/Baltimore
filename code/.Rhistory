black_articles_decade <- zz %>%
select(decade, filename) %>%
group_by(decade) %>%
count() %>%
mutate(Pct_Total =formattable::percent(round(n/1045,2)))
library(kableExtra)
black_articles_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_word_decade %>%
ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Black Press Avg Word Count in Lynching News Coverage",
subtitle = "Based in 1,045 extracted articles, 1892-2002",
caption = "Graphic by Rob Wells, 12-26-2023",
y="Average Article Word Count",
x="Decade")
#ggsave("../output_images_tables/FigureX_black_press_avg_word_count_ap_19.png",device = "png",width=9,height=6, dpi=800)
View(zz)
View(black)
onlybptext <- filter(yy, grepl("bp", filename))
View(onlybptext)
write.csv(onlybptext("../data/only_bp_text.csv"))
write_csv(onlybptext("../data/only_bp_text.csv"))
write.csv(onlybptext,("../data/only_bp_text.csv"))
b <- stringi::stri_count_words(onlybptext$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
View(b)
onlybytext <- cbind(onlybptext, b)
bb <- onlybytext %>%
select(filename, sentence, words, year, newspaper_name, url)
names(onlybptext)
#65257 rows only the bp files.
onlybptext <- filter(black, grepl("bp", filename))
write.csv(onlybptext,("../data/only_bp_text.csv"))
b <- stringi::stri_count_words(onlybptext$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
onlybytext <- cbind(onlybptext, b)
bb <- onlybytext %>%
select(filename, sentence, words, year, newspaper_name, url)
names(onlybptext)
bb <- onlybptext %>%
select(filename, sentence, words, year, newspaper_name, url)
#
# # append decade information for aggregation
bb$decade <- paste0(substr(bb$year, 0, 3), "0")
yyy <- cbind(bb, y)
yyy <- rbind(bb, y)
zzz <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
zzz$file_id <- gsub('.txt',"", zzz$filename)
library(tidyr)
zzz <- separate(data = zzz, col = filename, into = c('fn1', 'crap'), sep = '_', extra ='merge', fill = 'right')
zzz$fn1 <- as.numeric(zzz$fn1)
zzz <- subset(zzz, select =-crap)
View(zzz)
yyy <- rbind(bb, y)
#MAKE A NEW DF THAT GROUPS BY FILE NAME, SUMMARIZES WORD COUNT SO EACH ARTICLE HAS A WORD COUNT
zzz <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words)) %>%
ungroup()
View(zzz)
z <- yyy %>%
select(filename, newspaper_name, url, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(z)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade==1960)
View(lynch_word_decade)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
View(zzz)
View(yyy)
lynch_word_decade %>%
ggplot(aes(x = decade, y = avg_words,fill = avg_words)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Average Word Count in Lynching News Coverage",
subtitle = "Based in 7,162 extracted articles, 1805-1969",
caption = "Graphic by Rob Wells, 12-26-2023",
y="Average Article Word Count",
x="Decade")
# ggsave("../output_images_tables/FigureX_avg_word_count_ap_16.png",device = "png",width=9,height=6, dpi=800)
ggsave("../output_images_tables/FigureX_avg_word_count_dec_26.png",device = "png",width=9,height=6, dpi=800)
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = count(decade))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = summarise(count(decade)))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = summarise(count=decade()))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
mutate(stories_decade = count=decade())
glimpse(z)
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
group_by(decade) %>%
mutate(stories_decade = count(decade)) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
# group_by(decade) %>%
mutate(stories_decade = count(decade)) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0))
z %>%
select(decade, total) %>%
# group_by(decade) %>%
mutate(stories_decade = count(decade))
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
)
z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
) %>%
mutate(avg_words = round(avg_words, 0))
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n()
) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
View(lynch_word_decade)
mean(lynch_word_decade$avg_words)
median(lynch_word_decade$avg_words)
black <- bp_text
xx <- stringi::stri_count_words(black$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
black <- cbind(black, xx)
View(black)
yy <- black %>%
select(filename, sentence, words, year)
# append decade information for aggregation
yy$decade <- paste0(substr(yy$year, 0, 3), "0")
zz <- yy %>%
select(filename, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0))
View(black_word_decade)
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade>1960)
mean(black_word_decade$avg_words)
sum(black_word_decade$num_articles)
black_word_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
black_word_decade <- zz %>%
select(decade, total) %>%
group_by(decade) %>%
summarise(
avg_words = mean(total, na.rm = TRUE),
num_articles = n() )%>%
mutate(avg_words = round(avg_words, 0))
library(kableExtra)
# black_articles_decade %>%
black_word_decade %>%
kbl(caption = "Black Press Article Totals", font_size = 30) %>%
kable_classic(full_width = F, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, width = "5em", background = "yellow")
library(reticulate)
reticulate::repl_python()
py_install("boto3")
reticulate::repl_python()
py_install("pillow")
reticulate::repl_python()
install.packages("retriculate")
library(reticulate)
install.packages("reticulate")
library(reticulate)
py_install("pillow")
# use_python("/path/to/your/python")
reticulate::repl_python()
library(reticulate)
py_install("pillow")
# use_python("/path/to/your/python")
reticulate::repl_python()
setwd("~/Code/data_journalism_class/03_tutorials/qmd_files")
quarto_render
quarto::quarto_render()
getwd()
setwd("~/Code/Baltimore/code")
#remotes::install_github("walkerke/tidycensus")
library(tidyverse)
library(tidycensus)
#a = get_decennial(geography = "state", variables = "P1_001N", year = 2020)
# head(a, 5)
#install.packages("formattable")
library(formattable)
library(htmlwidgets)
library(leaflet)
library(sf)
library(formattable)
library(dplyr)
library(tidyr)
library(janitor)
#install.packages("leaflet.extras")
library(leaflet.extras)
library(googlesheets4)
census_api_key("9cabe8a191a1f824755d4a1845f13cb08faa2c5f", install = TRUE, overwrite = TRUE)
smith <- read.csv("../data/smith1_balt.csv")
#199 census tracts
#25 zip codes (totalling 199 areas, but need to check boundaries)
x<- smith %>%
group_by(zip) %>%
count ()
sum(x$n)
#per capita income seems way too low
median(smith$income_per_capita, na.rm = TRUE) #22789.76
median(smith$pct_anymembershp_zip, na.rm = TRUE) #0.455563
summary(smith$pct_anymembershp_zip, na.rm = TRUE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
#  0.3353  0.4444  0.4556  0.4613  0.4752  0.5241       3
median(smith$nbanks_zip, na.rm = TRUE) #5
summary(smith$nbanks_zip, na.rm = TRUE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
# 0.000   2.000   5.000   5.847   8.000  14.000       3
median(smith$census_response_rate2020, na.rm = TRUE) #0.564
summary(smith$census_response_rate2020, na.rm = TRUE)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's
#  0.2830  0.4753  0.5640  0.5594  0.6522  0.8360       3
View(smith)
#communities with greater than median membership - 46%- but less than median income - $22,790
himember <- smith %>%
group_by(neighborhood) %>%
select(pct_anymembershp_zip, income_per_capita, tract) %>%
arrange(desc(pct_anymembershp_zip)) %>%
filter(pct_anymembershp_zip >= 0.455563) %>%
filter(income_per_capita <= 22789.76) %>%
mutate(pct_anymembershp_zip = formattable::percent(pct_anymembershp_zip)) %>%
himember$income_per_capita <- formattable::currency(himember$income_per_capita, digits =0L)
#communities with greater than median membership - 46%- but less than median income - $22,790
himember <- smith %>%
group_by(neighborhood) %>%
select(pct_anymembershp_zip, income_per_capita, tract) %>%
arrange(desc(pct_anymembershp_zip)) %>%
filter(pct_anymembershp_zip >= 0.455563) %>%
filter(income_per_capita <= 22789.76) %>%
mutate(pct_anymembershp_zip = formattable::percent(pct_anymembershp_zip))
himember$income_per_capita <- formattable::currency(himember$income_per_capita, digits =0L)
library(kableExtra)
himember %>%
rename(Neighborhood = neighborhood, Membership = pct_anymembershp_zip, Per_Capita_Income = income_per_capita, Census_Tract = tract) %>%
kbl(caption = "Communities With Greater than Median Membership - 46%- but Less than Median Income - $22,790", font_size = 50, bold=T) %>%
kable_classic(full_width = T, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, border_right = T, width = "10em", background = "yellow") %>%
column_spec(3, border_right = T, width = "10em") %>%
save_kable(file = "../DataOutput/himember.html", self_contained = T)
hicensus <- smith %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, tract) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(income_per_capita <= 22653.77) %>%
mutate(census_response_rate2020 = formattable::percent(census_response_rate2020)) %>%
mutate(pct_anymembershp_zip = formattable::percent(pct_anymembershp_zip))
hicensus$income_per_capita <- formattable::currency(hicensus$income_per_capita, digits =0L)
#communities in the upper quartile of census response - 65%- and upper quartile of membership - 46%
smith %>%
group_by(neighborhood) %>%
select(census_response_rate2020, income_per_capita, pct_anymembershp_zip, nbanks_zip) %>%
arrange(desc(census_response_rate2020)) %>%
filter(census_response_rate2020 >= 0.6522) %>%
filter(pct_anymembershp_zip >= 0.4613) %>%
arrange((income_per_capita))
knitr::opts_chunk$set(echo = TRUE)
# For general data science
library(tidyverse)
# For data cleaning
library(janitor)
# For working with datetime
library(lubridate)
# For pretty tables
library(kableExtra)
library(knitr)
#For map
library(tigris)
library(sf)
library(censusxy)
install.packages("censusxy")
library(censusxy)
remotes::install_github("chris-prener/censusxy")
library(censusxy)
library(tidycensus)
#install.packages("leaflet")
library(leaflet)
all_geocoded_retailers <- readRDS("../../data/processed/03_join_retailers/all_geocoded_retailers.rds")
fb <- rio::import("/Users/gizmofo/Downloads/us-zip-code-us-zip-code-fb-social-connectedness-index-october-2021/zcta_zcta_shard2.tsv")
View(fb)
baltimore_zips <- c("21201","21202","21203","21205","21206","21211","21213","21214","21215","21216","21217","21218","21222","21223","21224","21229","21230","21231","21234","21236","21237","21239","21251","21263","21264","21270","21273","21275","21278","21280","21281","21282","21286","21287","21288")
bmore_fb <- fb %>%
filter(user_loc = baltimore_zips)
bmore_fb <- fb %>%
filter(user_loc == baltimore_zips)
View(bmore_fb)
names(bmore_fb)
#"user_loc"   "fr_loc"     "scaled_sci"
summary(bmore_fb$scaled_sci)
summary(fb$scaled_sci)
bmore_fb %>%
count(user_loc) %>%
arrange(desc(n))
bmore_fb %>%
count(fr_loc) %>%
arrange(desc(n))
zip <- rio::import("../data/Zip-to-CSA-2010.xls")
View(zip)
#find top zip code neighborhoods by sba loan
#zip to neighborhood file
zip <- rio::import("../data/Zip-to-CSA-2010.xls")
zip %>%
count(Zip2010) %>%
arrange(desc(n))
bmore_fb %>%
right_join(zip, by=c("user_loc"="Zip2010"))
x< - bmore_fb %>%
right_join(zip, by=c("user_loc"="Zip2010")) %>%
distinct()
x <- bmore_fb %>%
right_join(zip, by=c("user_loc"="Zip2010")) %>%
distinct()
x <- bmore_fb %>%
inner_join(zip, by=c("user_loc"="Zip2010")) %>%
distinct()
x <- bmore_fb %>%
left_join(zip, by=c("user_loc"="Zip2010")) %>%
distinct()
df <- zip %>%
group_by(Zip2010) %>%
summarise(CSA2010 = toString(CSA2010))
View(df)
df <- zip %>%
group_by(Zip2010) %>%
summarise(CSA2010 = toString(CSA2010)) %>%
mutate(place = str_extract(CSA2010, "^[^,]*"))
View(df)
x <- bmore_fb %>%
left_join(df, by=c("user_loc"="Zip2010")) %>%
distinct()
x <- bmore_fb %>%
left_join(df, by=c("user_loc"="Zip2010"))
View(x)
bmore_fb <- bmore_fb %>%
left_join(df, by=c("user_loc"="Zip2010"))
bmore_fb %>%
count(place) %>%
arrange(desc(n))
bmore_fb %>%
select(place, scaled_sci) %>%
summarize(median_score =(median(scaled_sci)))
bmore_fb %>%
select(place, scaled_sci) %>%
summarize(median_score =(median(scaled_sci))) %>%
ungroup()
bmore_fb %>%
group_by(place) %>%
summarize(median_score =(median(scaled_sci))) %>%
ungroup()
bmore_fb %>%
group_by(place) %>%
summarize(median_score =(median(scaled_sci))) %>%
ungroup() %>%
arrange(desc(median_score))
bmore_fb %>%
group_by(place) %>%
summarize(median_score =(median(scaled_sci))) %>%
summarize(total_score =(sum(scaled_sci))) %>%
ungroup() %>%
arrange(desc(total_score))
bmore_fb %>%
group_by(place) %>%
summarize(
median_score = median(scaled_sci),
total_score = sum(scaled_sci)
)  %>%
ungroup() %>%
arrange(desc(total_score))
bmore_fb %>%
select(place, CSA2010) %>%
group_by(place) %>%
summarize(
median_score = median(scaled_sci),
total_score = sum(scaled_sci)
)  %>%
ungroup() %>%
arrange(desc(total_score))
bmore_fb %>%
select(place, CSA2010, scaled_sci) %>%
group_by(place) %>%
summarize(
median_score = median(scaled_sci),
total_score = sum(scaled_sci)
)  %>%
ungroup() %>%
arrange(desc(total_score))
bmore_fb %>%
group_by(place) %>%
summarize(
median_score = median(scaled_sci),
total_score = sum(scaled_sci),
zip = first(CSA2010) # Take first zip
)  %>%
ungroup() %>%
arrange(desc(total_score))
View(bmore_fb)
bmore_fb %>%
group_by(place) %>%
summarize(
median_score = median(scaled_sci),
total_score = sum(scaled_sci),
zip = first(user_loc) # Take first zip
)  %>%
ungroup() %>%
arrange(desc(total_score))
bmore_fb_summary <- bmore_fb %>%
group_by(place) %>%
summarize(
median_score = median(scaled_sci),
total_score = sum(scaled_sci),
zip = first(user_loc) # Take first zip
)  %>%
ungroup() %>%
arrange(desc(total_score))
bmore_fb_summary  <- bmore_fb_summary  %>%
sf::st_transform('+proj=longlat +datum=WGS84')
write.csv(bmore_fb_summary, "../Data_Output/bmore_fb_summary.csv")
write.csv(bmore_fb_summary, "../DataOutput/bmore_fb_summary.csv")
View(bmore_fb_summary)
library(kableExtra)
bmore_fb_summary %>%
kbl(caption = "Facebook Network Activity by Baltimore Zip Code", font_size = 50, bold=T) %>%
kable_classic(full_width = T, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, border_right = T, width = "10em", background = "yellow") %>%
column_spec(3, border_right = T, width = "10em") %>%
save_kable(file = "../output/bmore_fb_summary.html", self_contained = T)
library(kableExtra)
bmore_fb_summary %>%
kbl(caption = "Facebook Network Activity by Baltimore Zip Code", font_size = 50, bold=T) %>%
kable_classic(full_width = T, html_font = "Cambria") %>%
column_spec(1, bold = T, border_right = T) %>%
column_spec(2, border_right = T, width = "10em") %>%
column_spec(3, border_right = T, width = "10em", background = "yellow") %>%
save_kable(file = "../output/bmore_fb_summary.html", self_contained = T)
